name: Deploy to AWS

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  AWS_REGION: us-east-1
  ECR_API_REPO: proyecto1-api
  ECR_WORKER_REPO: proyecto1-worker
  ECR_FRONTEND_REPO: proyecto1-frontend

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | \
            docker login --username AWS --password-stdin ${{ secrets.ECR_REGISTRY }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push API image
        uses: docker/build-push-action@v5
        with:
          context: ./api
          platforms: linux/amd64
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_API_REPO }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_API_REPO }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push Worker image
        uses: docker/build-push-action@v5
        with:
          context: ./worker
          platforms: linux/amd64
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_WORKER_REPO }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_WORKER_REPO }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
      - name: Build and push Frontend image
        uses: docker/build-push-action@v5
        with:
          context: ./front
          platforms: linux/amd64
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_FRONTEND_REPO }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_FRONTEND_REPO }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

  migrate-database:
    needs: build-and-push
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install PostgreSQL client
        run: sudo apt-get install -y postgresql-client

      - name: Run database migrations
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          # Run migrations (ignore errors for existing tables)
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/001_create_user_table.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/002_create_video_table.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/003_add_is_public_to_videos.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/004_create_votes_table.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/005_create_player_rankings_view.sql || true

  build-frontend:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"
          cache: "npm"
          cache-dependency-path: front/package-lock.json

      - name: Install dependencies
        run: |
          cd front
          npm ci

      - name: Build frontend
        run: |
          cd front
          npm run build

      - name: Upload frontend artifact
        uses: actions/upload-artifact@v4
        with:
          name: frontend-dist
          path: front/dist

  deploy:
    needs: [build-and-push, migrate-database, build-frontend]
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy to Web Server (ASG)
        run: |
          # Get all instance IDs from the Auto Scaling Group
          ASG_NAME="${{ secrets.ASG_NAME }}"
          if [ -z "$ASG_NAME" ]; then
            ASG_NAME="proyecto1-web-server-asg"
          fi
          
          echo "Finding instances in ASG: $ASG_NAME"
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "$ASG_NAME" \
            --region ${{ env.AWS_REGION }} \
            --query 'AutoScalingGroups[0].Instances[*].InstanceId' \
            --output text)
          
          if [ -z "$INSTANCE_IDS" ] || [ "$INSTANCE_IDS" = "None" ]; then
            echo "‚ùå No instances found in ASG. Waiting for instances to be ready..."
            exit 1
          fi
          
          echo "Found instances: $INSTANCE_IDS"
          
          # Deploy to each instance using SSM
          for INSTANCE_ID in $INSTANCE_IDS; do
            echo "Deploying to instance: $INSTANCE_ID"
            
            # Wait for instance to be ready for SSM (can take 1-2 minutes after launch)
            echo "Waiting for instance $INSTANCE_ID to be ready for SSM..."
            MAX_WAIT=300  # 5 minutes
            WAIT_COUNT=0
            while [ $WAIT_COUNT -lt $MAX_WAIT ]; do
              PING_STATUS=$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
                --region ${{ env.AWS_REGION }} \
                --query 'InstanceInformationList[0].PingStatus' \
                --output text 2>/dev/null)
              
              if [ "$PING_STATUS" = "Online" ]; then
                echo "‚úÖ Instance $INSTANCE_ID is ready for SSM"
                break
              else
                WAIT_COUNT=$((WAIT_COUNT + 10))
                if [ $WAIT_COUNT -lt $MAX_WAIT ]; then
                  echo "‚è≥ Waiting for SSM agent to be ready... ($WAIT_COUNT/$MAX_WAIT seconds) [Status: ${PING_STATUS:-NotRegistered}]"
                  sleep 10
                else
                  echo "‚ùå Instance $INSTANCE_ID did not become ready for SSM within $MAX_WAIT seconds"
                  echo "   Current status: ${PING_STATUS:-NotRegistered}"
                  echo "   This usually means the IAM role doesn't have AmazonSSMManagedInstanceCore policy"
                  exit 1
                fi
              fi
            done
            
            # Upload nginx config via SSM
            echo "Uploading nginx config..."
            # Base64 encode the nginx config file
            NGINX_B64=$(cat nginx/nginx.aws.conf | base64 -w 0)
            
            # Create temporary file for SSM parameters (using file:// to avoid shell escaping issues)
            NGINX_PARAMS_FILE=$(mktemp)
            # Use jq to safely construct JSON - jq will properly escape the base64 content
            jq -n \
              --arg cmd1 "mkdir -p /home/ec2-user/proyecto1" \
              --arg cmd2 "echo '$NGINX_B64' | base64 -d > /home/ec2-user/proyecto1/nginx.aws.conf" \
              --arg cmd3 "chmod 644 /home/ec2-user/proyecto1/nginx.aws.conf" \
              --arg cmd4 "test -f /home/ec2-user/proyecto1/nginx.aws.conf && echo 'Nginx config uploaded successfully' || echo 'Nginx config upload failed'" \
              '{commands: [$cmd1, $cmd2, $cmd3, $cmd4]}' > "$NGINX_PARAMS_FILE"
            
            # Upload nginx config using SSM with base64 content embedded
            NGINX_UPLOAD_CMD_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters file://"$NGINX_PARAMS_FILE" \
              --region ${{ env.AWS_REGION }} \
              --query 'Command.CommandId' \
              --output text)
            
            # Clean up temp file
            rm -f "$NGINX_PARAMS_FILE"
            
            # Wait for nginx config upload to complete
            if [ -n "$NGINX_UPLOAD_CMD_ID" ]; then
              echo "Waiting for nginx config upload..."
              aws ssm wait command-executed \
                --command-id "$NGINX_UPLOAD_CMD_ID" \
                --instance-id "$INSTANCE_ID" \
                --region ${{ env.AWS_REGION }} || echo "Warning: Nginx config upload may have failed"
              
              # Verify upload succeeded
              UPLOAD_STATUS=$(aws ssm get-command-invocation \
                --command-id "$NGINX_UPLOAD_CMD_ID" \
                --instance-id "$INSTANCE_ID" \
                --region ${{ env.AWS_REGION }} \
                --query 'Status' \
                --output text)
              
              if [ "$UPLOAD_STATUS" != "Success" ]; then
                echo "‚ö†Ô∏è  Nginx config upload failed, but continuing deployment..."
                aws ssm get-command-invocation \
                  --command-id "$NGINX_UPLOAD_CMD_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --region ${{ env.AWS_REGION }} \
                  --query 'StandardErrorContent' \
                  --output text || true
              else
                echo "‚úÖ Nginx config uploaded successfully"
              fi
            fi
            
            # Note: Frontend files are already included in the Docker image, so no need to download them separately.
            # The nginx config proxies to the frontend container (localhost:3000), which serves the built files.
            
            # Send deployment command via SSM
            # Create temporary file for SSM parameters (using file:// to avoid shell escaping issues)
            DEPLOY_PARAMS_FILE=$(mktemp)
            # Build parameters JSON using jq
            jq -n \
              --arg cmd1 "set -e" \
              --arg cmd2 "aws ecr get-login-password --region ${{ env.AWS_REGION }} | sudo docker login --username AWS --password-stdin ${{ secrets.ECR_REGISTRY }}" \
              --arg cmd3 "sudo docker stop proyecto1-api-aws || true" \
              --arg cmd4 "sudo docker rm proyecto1-api-aws || true" \
              --arg cmd5 "sudo docker pull ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_API_REPO }}:latest" \
              --arg cmd6 "sudo docker run -d --name proyecto1-api-aws --restart unless-stopped --network host -e DB_HOST=${{ secrets.RDS_ENDPOINT }} -e DB_PORT=5432 -e DB_NAME=proyecto_1 -e DB_USER=postgres -e DB_PASSWORD=${{ secrets.DB_PASSWORD }} -e DB_SSL_MODE=require -e S3_BUCKET_NAME=${{ secrets.S3_BUCKET_NAME }} -e SQS_QUEUE_NAME=${{ secrets.SQS_QUEUE_NAME }} -e JWT_SECRET=${{ secrets.JWT_SECRET }} -e AWS_DEFAULT_REGION=${{ env.AWS_REGION }} ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_API_REPO }}:latest" \
              --arg cmd7 "sudo docker stop proyecto1-frontend-aws || true" \
              --arg cmd8 "sudo docker rm proyecto1-frontend-aws || true" \
              --arg cmd9 "sudo docker pull ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_FRONTEND_REPO }}:latest" \
              --arg cmd10 "sudo docker run -d --name proyecto1-frontend-aws --restart unless-stopped --network host ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_FRONTEND_REPO }}:latest" \
              --arg cmd11 "sudo docker stop proyecto1-nginx-aws || true" \
              --arg cmd12 "sudo docker rm proyecto1-nginx-aws || true" \
              --arg cmd13 "sudo docker run -d --name proyecto1-nginx-aws --restart unless-stopped --network host -v /home/ec2-user/proyecto1/nginx.aws.conf:/etc/nginx/nginx.conf:ro nginx:alpine" \
              --arg cmd14 "sudo docker ps" \
              --arg timeout "3600" \
              --arg workdir "/home/ec2-user" \
              '{commands: [$cmd1, $cmd2, $cmd3, $cmd4, $cmd5, $cmd6, $cmd7, $cmd8, $cmd9, $cmd10, $cmd11, $cmd12, $cmd13, $cmd14], executionTimeout: [$timeout], workingDirectory: [$workdir]}' > "$DEPLOY_PARAMS_FILE"
            
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters file://"$DEPLOY_PARAMS_FILE" \
              --region ${{ env.AWS_REGION }} \
              --query 'Command.CommandId' \
              --output text)
            
            # Clean up temp file
            rm -f "$DEPLOY_PARAMS_FILE"
            
            echo "Command ID: $COMMAND_ID"
            
            # Wait for command to complete
            echo "Waiting for deployment to complete..."
            aws ssm wait command-executed \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" \
              --region ${{ env.AWS_REGION }} || true
            
            # Check command status
            STATUS=$(aws ssm get-command-invocation \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" \
              --region ${{ env.AWS_REGION }} \
              --query 'Status' \
              --output text)
            
            if [ "$STATUS" != "Success" ]; then
              echo "‚ùå Deployment failed on instance $INSTANCE_ID with status: $STATUS"
              aws ssm get-command-invocation \
                --command-id "$COMMAND_ID" \
                --instance-id "$INSTANCE_ID" \
                --region ${{ env.AWS_REGION }} \
                --query 'StandardErrorContent' \
                --output text
              exit 1
            else
              echo "‚úÖ Deployment successful on instance $INSTANCE_ID"
            fi
          done
          
          echo "‚úÖ All instances deployed successfully!"

      - name: Restart Worker
        run: |
          # Get worker instance ID by tag
          echo "Finding worker instance..."
          WORKER_INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=proyecto1-worker" "Name=instance-state-name,Values=running" \
            --region ${{ env.AWS_REGION }} \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text)
          
          if [ -z "$WORKER_INSTANCE_ID" ] || [ "$WORKER_INSTANCE_ID" = "None" ]; then
            echo "‚ùå Worker instance not found"
            exit 1
          fi
          
          echo "Found worker instance: $WORKER_INSTANCE_ID"
          
          # Check if IAM role has SSM policy attached
          echo "Checking IAM role permissions..."
          INSTANCE_PROFILE_ARN=$(aws ec2 describe-instances \
            --instance-ids "$WORKER_INSTANCE_ID" \
            --region ${{ env.AWS_REGION }} \
            --query 'Reservations[0].Instances[0].IamInstanceProfile.Arn' \
            --output text)
          
          if [ -n "$INSTANCE_PROFILE_ARN" ] && [ "$INSTANCE_PROFILE_ARN" != "None" ]; then
            INSTANCE_PROFILE_NAME=$(echo "$INSTANCE_PROFILE_ARN" | cut -d'/' -f2)
            WORKER_ROLE_NAME=$(aws iam get-instance-profile \
              --instance-profile-name "$INSTANCE_PROFILE_NAME" \
              --query 'InstanceProfile.Roles[0].RoleName' \
              --output text 2>/dev/null)
          else
            WORKER_ROLE_NAME="proyecto1-worker-role"  # Fallback to expected name
          fi
          
          if [ -n "$WORKER_ROLE_NAME" ] && [ "$WORKER_ROLE_NAME" != "None" ]; then
            echo "Worker IAM role: $WORKER_ROLE_NAME"
            # Check if SSM policy is attached
            SSM_POLICY_ATTACHED=$(aws iam list-attached-role-policies \
              --role-name "$WORKER_ROLE_NAME" \
              --query 'AttachedPolicies[?PolicyArn==`arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore`]' \
              --output text 2>/dev/null)
            
            if [ -z "$SSM_POLICY_ATTACHED" ]; then
              echo "‚ö†Ô∏è  SSM policy not attached to worker role. Attempting to attach..."
              if aws iam attach-role-policy \
                --role-name "$WORKER_ROLE_NAME" \
                --policy-arn "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore" 2>/dev/null; then
                echo "‚úÖ Successfully attached SSM policy"
                echo "‚è≥ Waiting 30 seconds for policy to propagate..."
                sleep 30
              else
                echo "‚ö†Ô∏è  Failed to attach policy automatically (may need manual intervention or Terraform apply)"
              fi
            else
              echo "‚úÖ SSM policy is attached to worker role"
            fi
          fi
          
          # Wait for instance to be ready for SSM
          echo "Waiting for worker instance to be ready for SSM..."
          MAX_WAIT=300  # 5 minutes
          WAIT_COUNT=0
          while [ $WAIT_COUNT -lt $MAX_WAIT ]; do
            PING_STATUS=$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=$WORKER_INSTANCE_ID" \
              --region ${{ env.AWS_REGION }} \
              --query 'InstanceInformationList[0].PingStatus' \
              --output text 2>/dev/null)
            
            if [ "$PING_STATUS" = "Online" ]; then
              echo "‚úÖ Worker instance is ready for SSM"
              break
            else
              WAIT_COUNT=$((WAIT_COUNT + 10))
              if [ $WAIT_COUNT -lt $MAX_WAIT ]; then
                echo "‚è≥ Waiting for SSM agent to be ready... ($WAIT_COUNT/$MAX_WAIT seconds) [Status: ${PING_STATUS:-NotRegistered}]"
                sleep 10
              else
                echo "‚ùå Worker instance did not become ready for SSM within $MAX_WAIT seconds"
                echo "   Current status: ${PING_STATUS:-NotRegistered}"
                echo ""
                echo "   To fix this issue:"
                echo "   1. Apply Terraform changes: cd terraform && terraform apply"
                echo "   2. Or manually attach policy:"
                echo "      aws iam attach-role-policy --role-name proyecto1-worker-role --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
                echo "   3. Then reboot the instance:"
                echo "      aws ec2 reboot-instances --instance-ids $WORKER_INSTANCE_ID"
                exit 1
              fi
            fi
          done
          
          # Create temporary file for SSM parameters
          WORKER_PARAMS_FILE=$(mktemp)
          jq -n \
            --arg cmd1 "aws ecr get-login-password --region ${{ env.AWS_REGION }} | sudo docker login --username AWS --password-stdin ${{ secrets.ECR_REGISTRY }}" \
            --arg cmd2 "sudo docker stop proyecto1-worker-aws || true" \
            --arg cmd3 "sudo docker rm proyecto1-worker-aws || true" \
            --arg cmd4 "sudo docker pull ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_WORKER_REPO }}:latest" \
            --arg cmd5 "sudo docker run -d --name proyecto1-worker-aws --restart unless-stopped --network host -e DB_HOST=${{ secrets.RDS_ENDPOINT }} -e DB_PORT=5432 -e DB_NAME=proyecto_1 -e DB_USER=postgres -e DB_PASSWORD=${{ secrets.DB_PASSWORD }} -e DB_SSL_MODE=require -e S3_BUCKET_NAME=${{ secrets.S3_BUCKET_NAME }} -e SQS_QUEUE_NAME=${{ secrets.SQS_QUEUE_NAME }} -e AWS_DEFAULT_REGION=${{ env.AWS_REGION }} ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_WORKER_REPO }}:latest" \
            --arg cmd6 "sudo docker ps" \
            '{commands: [$cmd1, $cmd2, $cmd3, $cmd4, $cmd5, $cmd6]}' > "$WORKER_PARAMS_FILE"
          
          # Send restart command via SSM
          echo "Restarting worker via SSM..."
          WORKER_COMMAND_ID=$(aws ssm send-command \
            --instance-ids "$WORKER_INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --parameters file://"$WORKER_PARAMS_FILE" \
            --region ${{ env.AWS_REGION }} \
            --query 'Command.CommandId' \
            --output text)
          
          # Clean up temp file
          rm -f "$WORKER_PARAMS_FILE"
          
          echo "Worker command ID: $WORKER_COMMAND_ID"
          
          # Wait for command to complete
          echo "Waiting for worker restart to complete..."
          aws ssm wait command-executed \
            --command-id "$WORKER_COMMAND_ID" \
            --instance-id "$WORKER_INSTANCE_ID" \
            --region ${{ env.AWS_REGION }} || true
          
          # Check command status
          WORKER_STATUS=$(aws ssm get-command-invocation \
            --command-id "$WORKER_COMMAND_ID" \
            --instance-id "$WORKER_INSTANCE_ID" \
            --region ${{ env.AWS_REGION }} \
            --query 'Status' \
            --output text)
          
          if [ "$WORKER_STATUS" != "Success" ]; then
            echo "‚ùå Worker restart failed with status: $WORKER_STATUS"
            aws ssm get-command-invocation \
              --command-id "$WORKER_COMMAND_ID" \
              --instance-id "$WORKER_INSTANCE_ID" \
              --region ${{ env.AWS_REGION }} \
              --query 'StandardErrorContent' \
              --output text
            exit 1
          else
            echo "‚úÖ Worker restarted successfully"
          fi

      - name: Health Check
        run: |
          # Get ALB DNS name from secret or AWS CLI
          ALB_DNS="${{ secrets.ALB_DNS_NAME }}"
          
          if [ -z "$ALB_DNS" ]; then
            echo "‚ö†Ô∏è  ALB_DNS_NAME secret not set. Attempting to get from AWS..."
            # Try to get ALB DNS from AWS CLI using the ASG name
            ASG_NAME="${{ secrets.ASG_NAME }}"
            if [ -z "$ASG_NAME" ]; then
              ASG_NAME="proyecto1-web-server-asg"
            fi
            
            # Get ALB DNS name from AWS
            ALB_DNS=$(aws elbv2 describe-load-balancers \
              --region ${{ env.AWS_REGION }} \
              --query "LoadBalancers[?contains(LoadBalancerName, 'proyecto1-web-server-alb')].DNSName" \
              --output text | head -n 1)
          fi
          
          if [ -z "$ALB_DNS" ]; then
            echo "‚ùå Could not determine ALB DNS name."
            echo "   Please set ALB_DNS_NAME secret in GitHub Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo "   Get the value with: terraform output -raw alb_dns_name"
            exit 1
          fi
          
          echo "üåê Checking health at: http://$ALB_DNS"
          echo "‚è≥ Waiting for containers to start and ALB targets to become healthy..."
          echo "   (This may take 60-90 seconds for first deployment)"
          sleep 60
          
          # Check ALB target health
          echo "Checking ALB target health..."
          ALB_ARN=$(aws elbv2 describe-load-balancers \
            --region ${{ env.AWS_REGION }} \
            --query "LoadBalancers[?contains(LoadBalancerName, 'proyecto1-web-server-alb')].LoadBalancerArn" \
            --output text | head -n 1)
          
          if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
            TARGET_GROUPS=$(aws elbv2 describe-target-groups \
              --region ${{ env.AWS_REGION }} \
              --query "TargetGroups[?contains(LoadBalancerArns[0], '$ALB_ARN')].TargetGroupArn" \
              --output text)
            
            for TG_ARN in $TARGET_GROUPS; do
              echo "  Target Group: $TG_ARN"
              aws elbv2 describe-target-health \
                --target-group-arn "$TG_ARN" \
                --region ${{ env.AWS_REGION }} \
                --query 'TargetHealthDescriptions[*].[Target.Id,TargetHealth.State,TargetHealth.Reason]' \
                --output table || true
            done
          fi
          
          # Retry health check with more attempts and better diagnostics
          MAX_RETRIES=6
          RETRY_COUNT=0
          RETRY_DELAY=20
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo ""
            echo "Attempt $((RETRY_COUNT + 1))/$MAX_RETRIES: Checking API health through ALB..."
            
            # Get HTTP status code and response
            HTTP_CODE=$(curl -s -o /tmp/health_response.json -w "%{http_code}" --max-time 15 "http://$ALB_DNS/api/health" || echo "000")
            
            if [ "$HTTP_CODE" = "200" ]; then
              echo "‚úÖ API health check passed (HTTP $HTTP_CODE)"
              cat /tmp/health_response.json | jq '.' 2>/dev/null || cat /tmp/health_response.json
              rm -f /tmp/health_response.json
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "‚ùå Health check failed (HTTP $HTTP_CODE)"
              if [ -f /tmp/health_response.json ]; then
                echo "Response:"
                cat /tmp/health_response.json
                echo ""
                rm -f /tmp/health_response.json
              fi
              
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "‚è≥ Retrying in $RETRY_DELAY seconds..."
                sleep $RETRY_DELAY
              else
                echo ""
                echo "‚ùå API health check failed after $MAX_RETRIES attempts"
                echo ""
                echo "Troubleshooting steps:"
                echo "1. Check if containers are running: SSH to instance and run 'sudo docker ps'"
                echo "2. Check container logs: 'sudo docker logs proyecto1-api-aws'"
                echo "3. Check ALB target health in AWS Console"
                echo "4. Verify database connectivity from the instance"
                echo "5. Check nginx logs: 'sudo docker logs proyecto1-nginx-aws'"
                exit 1
              fi
            fi
          done
          
          echo ""
          echo "Checking Frontend..."
          FRONTEND_CODE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "http://$ALB_DNS" || echo "000")
          if [ "$FRONTEND_CODE" = "200" ] || [ "$FRONTEND_CODE" = "304" ]; then
            echo "‚úÖ Frontend check passed (HTTP $FRONTEND_CODE)"
          else
            echo "‚ö†Ô∏è  Frontend check returned HTTP $FRONTEND_CODE (API is working, frontend may need more time)"
          fi
          
          echo ""
          echo "‚úÖ Deployment successful!"
          echo "üåê Application URL: http://$ALB_DNS"
