name: Deployment

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  AWS_REGION: us-east-1
  ECR_API_REPO: proyecto1-api
  ECR_WORKER_REPO: proyecto1-worker
  ECR_FRONTEND_REPO: proyecto1-frontend

jobs:
  build-and-push-to-ecr:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | \
            docker login --username AWS --password-stdin ${{ secrets.ECR_REGISTRY }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push API image
        uses: docker/build-push-action@v5
        with:
          context: ./api
          platforms: linux/amd64
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_API_REPO }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_API_REPO }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push Lambda Worker image
        uses: docker/build-push-action@v5
        with:
          context: ./worker
          file: ./worker/Dockerfile.lambda
          push: true
          provenance: false
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_WORKER_REPO }}:lambda-${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_WORKER_REPO }}:lambda
          cache-from: type=gha
          cache-to: type=gha,mode=max

  migrate-database:
    needs: build-and-push-to-ecr
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install PostgreSQL client
        run: sudo apt-get install -y postgresql-client

      - name: Run database migrations
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          # Run migrations (ignore errors for existing tables)
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/001_create_user_table.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/002_create_video_table.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/003_add_is_public_to_videos.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/004_create_votes_table.sql || true
          psql -h ${{ secrets.RDS_ENDPOINT }} -U postgres -d proyecto_1 -f db/005_create_player_rankings_view.sql || true

  build-and-deploy-frontend:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "18"
        cache: "npm"
        cache-dependency-path: front/package-lock.json
      
    - name: Install dependencies
      run: |
          cd front
          npm ci
      
    - name: Build frontend
      run: |
          cd front
          npm run build
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
    - name: Deploy to S3
      run: |
          aws s3 sync front/dist/ s3://${{ secrets.FRONTEND_S3_BUCKET }}/ \
            --delete \
            --cache-control "public,max-age=31536000,immutable" \
            --exclude "index.html"
          
          aws s3 cp front/dist/index.html s3://${{ secrets.FRONTEND_S3_BUCKET }}/index.html \
            --cache-control "public,max-age=0,must-revalidate"
      
    - name: Invalidate CloudFront cache
      run: |
          aws cloudfront create-invalidation \
            --distribution-id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} \
            --paths "/*"

  deploy:
    needs: [build-and-push-to-ecr, migrate-database, build-and-deploy-frontend]
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy to Web Server (ASG) - Rolling Update
        run: |
          # With automated user-data script, we just need to pull new images on existing instances
          # New instances will automatically get the latest images from ECR
          
          ASG_NAME="${{ secrets.ASG_NAME }}"
          if [ -z "$ASG_NAME" ]; then
            ASG_NAME="proyecto1-web-server-asg"
          fi
          
          echo "üîÑ Deploying new images to ASG: $ASG_NAME"
          echo "Finding instances in ASG..."
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "$ASG_NAME" \
            --region ${{ env.AWS_REGION }} \
            --query 'AutoScalingGroups[0].Instances[?LifecycleState==`InService`].InstanceId' \
            --output text)
          
          if [ -z "$INSTANCE_IDS" ] || [ "$INSTANCE_IDS" = "None" ]; then
            echo "‚ùå No instances found in ASG."
            exit 1
          fi
          
          echo "Found instances: $INSTANCE_IDS"
          
          # Simple rolling update: pull new images and restart containers
          for INSTANCE_ID in $INSTANCE_IDS; do
            echo "üîÑ Updating instance: $INSTANCE_ID"
            
            # Wait for SSM to be ready
            echo "Waiting for SSM agent..."
            MAX_WAIT=120
            WAIT_COUNT=0
            while [ $WAIT_COUNT -lt $MAX_WAIT ]; do
              PING_STATUS=$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
                --region ${{ env.AWS_REGION }} \
                --query 'InstanceInformationList[0].PingStatus' \
                --output text 2>/dev/null || echo "None")
              
              if [ "$PING_STATUS" = "Online" ]; then
                echo "‚úÖ SSM ready for $INSTANCE_ID"
                break
              else
                WAIT_COUNT=$((WAIT_COUNT + 10))
                if [ $WAIT_COUNT -lt $MAX_WAIT ]; then
                  echo "‚è≥ Waiting for SSM... ($WAIT_COUNT/$MAX_WAIT)"
                  sleep 10
                else
                  echo "‚ö†Ô∏è  SSM not ready for $INSTANCE_ID, skipping..."
                  continue 2
                fi
              fi
            done
            
            # Update containers with new images
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=[
                "set -e",
                "cd /home/ec2-user/proyecto1",
                "aws ecr get-login-password --region '"${{ env.AWS_REGION }}"' | sudo docker login --username AWS --password-stdin '"${{ secrets.ECR_REGISTRY }}"'",
                "sudo docker-compose pull",
                "sudo docker-compose up -d",
                "sudo docker-compose ps"
              ]' \
              --region ${{ env.AWS_REGION }} \
              --query 'Command.CommandId' \
              --output text 2>/dev/null || echo "FAILED")
            
            if [ "$COMMAND_ID" = "FAILED" ] || [ -z "$COMMAND_ID" ]; then
              echo "‚ö†Ô∏è  Could not send command to $INSTANCE_ID"
              continue
            fi
            
            echo "Waiting for update to complete..."
            aws ssm wait command-executed \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" \
              --region ${{ env.AWS_REGION }} 2>/dev/null || true
            
            STATUS=$(aws ssm get-command-invocation \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" \
              --region ${{ env.AWS_REGION }} \
              --query 'Status' \
              --output text 2>/dev/null || echo "Unknown")
            
            if [ "$STATUS" = "Success" ]; then
              echo "‚úÖ Instance $INSTANCE_ID updated successfully"
            else
              echo "‚ö†Ô∏è  Instance $INSTANCE_ID update status: $STATUS"
            fi
          done
          
          echo "‚úÖ Deployment complete!"

      - name: Update Lambda Worker Function
        run: |
          echo "üöÄ Updating Lambda worker function..."
          
          # Check if Lambda function exists
          LAMBDA_EXISTS=$(aws lambda get-function \
            --function-name proyecto1-video-processor \
            --region ${{ env.AWS_REGION }} 2>&1 || echo "NOT_FOUND")
          
          if echo "$LAMBDA_EXISTS" | grep -q "ResourceNotFoundException\|NOT_FOUND"; then
            echo "‚ö†Ô∏è  Lambda function not found. Skipping Lambda update."
            echo "   Run 'terraform apply' to create the Lambda function first."
          else
            echo "‚úÖ Lambda function exists. Updating with new image..."
            
            # Update Lambda function with new image
            aws lambda update-function-code \
              --function-name proyecto1-video-processor \
              --image-uri ${{ secrets.ECR_REGISTRY }}/${{ env.ECR_WORKER_REPO }}:lambda \
              --region ${{ env.AWS_REGION }}
            
            echo "‚è≥ Waiting for Lambda update to complete..."
            aws lambda wait function-updated \
              --function-name proyecto1-video-processor \
              --region ${{ env.AWS_REGION }}
            
            echo "‚úÖ Lambda function updated successfully!"
            
            # Get Lambda function info
            aws lambda get-function \
              --function-name proyecto1-video-processor \
              --region ${{ env.AWS_REGION }} \
              --query '{FunctionName:Configuration.FunctionName,Runtime:Configuration.PackageType,Timeout:Configuration.Timeout,Memory:Configuration.MemorySize,LastModified:Configuration.LastModified}' \
              --output table
          fi

      - name: Health Check
        run: |
          # Get ALB DNS name from secret or AWS CLI
          ALB_DNS="${{ secrets.ALB_DNS_NAME }}"
          
          if [ -z "$ALB_DNS" ]; then
            echo "‚ö†Ô∏è  ALB_DNS_NAME secret not set. Attempting to get from AWS..."
            # Try to get ALB DNS from AWS CLI using the ASG name
            ASG_NAME="${{ secrets.ASG_NAME }}"
            if [ -z "$ASG_NAME" ]; then
              ASG_NAME="proyecto1-web-server-asg"
            fi
            
            # Get ALB DNS name from AWS
            ALB_DNS=$(aws elbv2 describe-load-balancers \
              --region ${{ env.AWS_REGION }} \
              --query "LoadBalancers[?contains(LoadBalancerName, 'proyecto1-web-server-alb')].DNSName" \
              --output text | head -n 1)
          fi
          
          if [ -z "$ALB_DNS" ]; then
            echo "‚ùå Could not determine ALB DNS name."
            echo "   Please set ALB_DNS_NAME secret in GitHub Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo "   Get the value with: terraform output -raw alb_dns_name"
            exit 1
          fi
          
          echo "üåê Checking health at: http://$ALB_DNS"
          echo "‚è≥ Waiting for containers to start and ALB targets to become healthy..."
          echo "   (This may take 60-90 seconds for first deployment)"
          sleep 60
          
          # Check ALB target health
          echo "Checking ALB target health..."
          ALB_ARN=$(aws elbv2 describe-load-balancers \
            --region ${{ env.AWS_REGION }} \
            --query "LoadBalancers[?contains(LoadBalancerName, 'proyecto1-web-server-alb')].LoadBalancerArn" \
            --output text | head -n 1)
          
          if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
            TARGET_GROUPS=$(aws elbv2 describe-target-groups \
              --region ${{ env.AWS_REGION }} \
              --query "TargetGroups[?contains(LoadBalancerArns[0], '$ALB_ARN')].TargetGroupArn" \
              --output text)
            
            for TG_ARN in $TARGET_GROUPS; do
              echo "  Target Group: $TG_ARN"
              aws elbv2 describe-target-health \
                --target-group-arn "$TG_ARN" \
                --region ${{ env.AWS_REGION }} \
                --query 'TargetHealthDescriptions[*].[Target.Id,TargetHealth.State,TargetHealth.Reason]' \
                --output table || true
            done
          fi
          
          # Retry health check with more attempts and better diagnostics
          MAX_RETRIES=6
          RETRY_COUNT=0
          RETRY_DELAY=20
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo ""
            echo "Attempt $((RETRY_COUNT + 1))/$MAX_RETRIES: Checking API health through ALB..."
            
            # Get HTTP status code and response
            HTTP_CODE=$(curl -s -o /tmp/health_response.json -w "%{http_code}" --max-time 15 "http://$ALB_DNS/api/health" || echo "000")
            
            if [ "$HTTP_CODE" = "200" ]; then
              echo "‚úÖ API health check passed (HTTP $HTTP_CODE)"
              cat /tmp/health_response.json | jq '.' 2>/dev/null || cat /tmp/health_response.json
              rm -f /tmp/health_response.json
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "‚ùå Health check failed (HTTP $HTTP_CODE)"
              if [ -f /tmp/health_response.json ]; then
                echo "Response:"
                cat /tmp/health_response.json
                echo ""
                rm -f /tmp/health_response.json
              fi
              
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "‚è≥ Retrying in $RETRY_DELAY seconds..."
                sleep $RETRY_DELAY
              else
                echo ""
                echo "‚ùå API health check failed after $MAX_RETRIES attempts"
                echo ""
                echo "Troubleshooting steps:"
                echo "1. Check if containers are running: SSH to instance and run 'sudo docker ps'"
                echo "2. Check container logs: 'sudo docker logs proyecto1-api-aws'"
                echo "3. Check ALB target health in AWS Console"
                echo "4. Verify database connectivity from the instance"
                echo "5. Check nginx logs: 'sudo docker logs proyecto1-nginx-aws'"
                exit 1
              fi
            fi
          done
          
          echo ""
          echo "Checking Frontend..."
          FRONTEND_CODE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "http://$ALB_DNS" || echo "000")
          if [ "$FRONTEND_CODE" = "200" ] || [ "$FRONTEND_CODE" = "304" ]; then
            echo "‚úÖ Frontend check passed (HTTP $FRONTEND_CODE)"
          else
            echo "‚ö†Ô∏è  Frontend check returned HTTP $FRONTEND_CODE (API is working, frontend may need more time)"
          fi
          
          echo ""
          echo "‚úÖ Deployment successful!"
          echo "üåê Application URL: http://$ALB_DNS"
